{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMQpxhOhZbp/7NFbjN4QIaX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rganesh203/Azure-Data-Engineer-Road-map-2023-24/blob/main/Pyspark_Transformation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation Operations\n",
        "always RDD\n",
        "    1. Narrow transformation\n",
        "        map\n",
        "        flatMap\n",
        "        filter\n",
        "        mapPartitions\n",
        "        mapPartitionWithIndex\n",
        "        union\n",
        "    2. Wide transformation\n",
        "        intersection\n",
        "        distinct\n",
        "        groupByKey\n",
        "        reduceByKey\n",
        "        sortByKey\n",
        "        join\n",
        "        coalesce"
      ],
      "metadata": {
        "id": "vMK9wv7t34sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. df.take()"
      ],
      "metadata": {
        "id": "sSg3qAPV9CVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pyspark.sql.DataFrame.take\n",
        "DataFrame.take(num: int) → List[pyspark.sql.types.Row]\n",
        "Returns the first num rows as a list of Row.\n",
        "\n",
        "New in version 1.3.0.\n",
        "\n",
        "Changed in version 3.4.0: Supports Spark Connect.\n",
        "\n",
        "Parameters\n",
        "numint\n",
        "Number of records to return. Will return this number of records or all records if the DataFrame contains less than this number of records..\n",
        "\n",
        "Returns\n",
        "list\n",
        "List of rows\n",
        "\n",
        "Examples\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(14, \"Tom\"), (23, \"Alice\"), (16, \"Bob\")], [\"age\", \"name\"])\n",
        "Return the first 2 rows of the DataFrame.\n",
        "\n",
        "df.take(2)\n",
        "[Row(age=14, name='Tom'), Row(age=23, name='Alice')]"
      ],
      "metadata": {
        "id": "KmrpFjZS9FLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. df.Collect()"
      ],
      "metadata": {
        "id": "hnd7h68q9Gfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "collect() is an action operation that is used to retrieve all the elements of the dataset (from all nodes) to the driver node."
      ],
      "metadata": {
        "id": "wmLMEuEk9Yi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. df.show()"
      ],
      "metadata": {
        "id": "dhKqNciY-CV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " show() is used to display the contents of the DataFrame in a Table Row and Column Format. By default, it shows only 20 Rows, and the column values are truncated at 20 characters."
      ],
      "metadata": {
        "id": "48-8bQKD91dX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#take() and show() are different.\n",
        "#show() prints results, take() returns a list of rows (in PySpark) and can be used to create a new dataframe. They are both actions."
      ],
      "metadata": {
        "id": "ihOtnOuK-1zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.show() : It will show only the content of the dataframe.\n",
        "#df.collect() : It will show the content and metadata of the dataframe.\n",
        "#df.take() : shows content and structure/metadata for a limited number of rows for a very large dataset."
      ],
      "metadata": {
        "id": "8OVI8DeK-wal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. takeOrdered()"
      ],
      "metadata": {
        "id": "sFh0flacs7tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# syntax RDD.takeOrdered(num, key=None)\n",
        "#Get the N elements from an RDD ordered in ascending order or as specified by the optional key function.\n",
        "\n",
        "#This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory.\n",
        "\n",
        "sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
        "[1, 2, 3, 4, 5, 6]\n",
        "sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
        "[10, 9, 7, 6, 5, 4]"
      ],
      "metadata": {
        "id": "d-p9nya2tizI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Union"
      ],
      "metadata": {
        "id": "2NB6GGtItI9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "union() method of the DataFrame is used to merge two DataFrame’s of the same structure/schema. If schemas are not the same it returns an error."
      ],
      "metadata": {
        "id": "yrT3Fb3559o_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. saveAsTextFile"
      ],
      "metadata": {
        "id": "83kvoVt77NDD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax: RDD.saveAsTextFile(path: str, compressionCodecClass: Optional[str] = None)\n",
        "Save this RDD as a text file, using string representations of elements."
      ],
      "metadata": {
        "id": "6WNCRbwI7OlK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Spark 2.0 SparkSession is an entry point to underlying Spark functionality. All functionality available with SparkContext is also available in SparkSession. Also, it provides APIs to work on DataFrames and Datasets."
      ],
      "metadata": {
        "id": "ZZVdYwDKJ5Cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sparksession is child class of\n",
        "all the context availvable in the spark"
      ],
      "metadata": {
        "id": "RXl73TcCI5AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. flatMap"
      ],
      "metadata": {
        "id": "FT0gr3JVXEdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "flattens the RDD/DataFrame column after applying the function on every element and returns a new RDD/DataFrame respectively."
      ],
      "metadata": {
        "id": "c3MrxeyQSQab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#map is------>one to one generally\n",
        "#flatMap----->one to many it is faster"
      ],
      "metadata": {
        "id": "RUK5EOnuU9lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "map() – Spark map() transformation applies a function to each row in a DataFrame/Dataset and returns the new transformed Dataset.\n",
        "flatMap() – Spark flatMap() transformation flattens the DataFrame/Dataset after applying the function on every element and returns a new transformed Dataset. The returned Dataset will return more rows than the current DataFrame. It is also referred to as a one-to-many transformation function. This is one of the major differences between flatMap() and map()"
      ],
      "metadata": {
        "id": "Fa5ivs9JRyCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. reduceByKey()"
      ],
      "metadata": {
        "id": "cpj09pYVW_Mn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "reduceByKey() transformation is used to merge the values of each key using an associative reduce function.  It is a wider transformation as it shuffles data across multiple partitions and it operates on pair RDD (key/value pair)."
      ],
      "metadata": {
        "id": "acXjNB7NXCCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. sortByKey"
      ],
      "metadata": {
        "id": "deURaNMBXjXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark sortByKey() transformation is an RDD operation that is used to sort the values of the key by ascending or descending order. sortByKey() function operates on pair RDD (key/value pair)."
      ],
      "metadata": {
        "id": "2cO5uTIUXkuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Sample"
      ],
      "metadata": {
        "id": "9s4S4wtZXq32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Syntax:\n",
        "#sample(self, withReplacement, fraction, seed=None) #seed is ID\n",
        "#PySpark RDD also provides sample() function to get a random sampling, it also has another signature takeSample() that returns an Array[T]."
      ],
      "metadata": {
        "id": "sRtEC8era2pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD is python list\n",
        "#It doesnt have any table, Schema, column."
      ],
      "metadata": {
        "id": "7uyMw-nAffbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. intersect"
      ],
      "metadata": {
        "id": "kn93hB9vbRzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return a new DataFrame containing rows only in both this DataFrame and another DataFrame.\n",
        "\n",
        "This is equivalent to INTERSECT in SQL."
      ],
      "metadata": {
        "id": "8zYfkl5jbdoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12. takeOrdered"
      ],
      "metadata": {
        "id": "FWSOwmKtcNIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the N elements from an RDD ordered in ascending order or as specified by the optional key function.\n",
        "\n",
        "Notes\n",
        "\n",
        "This method should only be used if the resulting array is expected to be small, as all the data is loaded into the driver’s memory."
      ],
      "metadata": {
        "id": "eDkG6UXncUOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. Distinct"
      ],
      "metadata": {
        "id": "1dTG7nQVdWWT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to perform select distinct/unique rows from all columns use the distinct() method and to perform on a single column or multiple selected columns use dropDuplicates()."
      ],
      "metadata": {
        "id": "G7rLtu3-dbAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. sort by"
      ],
      "metadata": {
        "id": "enZzOUradbHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use either sort() or orderBy() function of PySpark DataFrame to sort DataFrame by ascending or descending order based on single or multiple columns"
      ],
      "metadata": {
        "id": "uFgJUDyJd3Or"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sort() is more efficient compared to orderBy() because the data is sorted on each partition individually and this is why the order in the output."
      ],
      "metadata": {
        "id": "gET4o35PeJhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15. mapPartitions"
      ],
      "metadata": {
        "id": "UeQkl1_Zef_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to map() PySpark mapPartitions() is a narrow transformation operation that applies a function to each partition of the RDD, if you have a DataFrame, you need to convert to RDD in order to use it. mapPartitions() is mainly used to initialize connections once for each partition instead of every row, this is the main difference between map() vs mapPartitions(). It is a narrow transformation as there will not be any data movement/shuffling between partitions to perform the function."
      ],
      "metadata": {
        "id": "Cc27n6I4x9MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#map is each row partition\n",
        "#mapPartitions overall partition\n",
        "#mapPartition is faster"
      ],
      "metadata": {
        "id": "2MDv5fkYAwuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return a new RDD by applying a function to each partition of this RDD."
      ],
      "metadata": {
        "id": "6j6x3e1Y3KUC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16. glom"
      ],
      "metadata": {
        "id": "WUqVcoaJ0rxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#syntax: RDD.glom() → pyspark.rdd.RDD[List[T]][source]\n",
        "Return an RDD created by coalescing all elements within each partition into a list.\n",
        "Returns\n",
        "RDD\n",
        "a new RDD coalescing all elements within each partition into a list"
      ],
      "metadata": {
        "id": "wDfeqH1e1AKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " glom() transforms each partition into a tuple (immutabe list) of elements. Creates an RDD of tules. One tuple per partition. workers can refer to elements of the partition by index. but you cannot assign values to the elements, the RDD is still immutable."
      ],
      "metadata": {
        "id": "80UhPVuF1pqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#narrow transformation and wide transformation in pyspark\n",
        "In summary, narrow transformations are operations where each input partition is used to\n",
        "compute one output partition, while wide transformations are operations where each input\n",
        "partition is used to compute multiple output partitions."
      ],
      "metadata": {
        "id": "y5fEEm1B4F0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Narrow transformations are operations where each input partition of an RDD is used to compute only one output partition of the resulting RDD. Examples of narrow transformations include map(), filter(), and union(). Narrow transformations are preferred because they allow for more efficient processing, as they can be executed in parallel on individual partitions without the need for shuffling or data movement across the cluster.\n",
        "\n",
        "Wide transformations, on the other hand, are operations where each input partition of an RDD is used to compute multiple output partitions of the resulting RDD. Examples of wide transformations include groupByKey(), reduceByKey(), and sortByKey(). Wide transformations require shuffling or data movement across the cluster to redistribute the data, which can be expensive in terms of performance and network overhead.\n",
        "\n",
        "- In wide transformation (shuffling ) is involved and so the data is written to disk thereby making it a costly and slow transformation .\n",
        "- In a DAG a new stage is created for every wide transformation.\n",
        "- for *optimisation* one must reduce usage of wide transformation if possible or atleast apply as many\n",
        "narrow transformations before proceeding to wide transformation.\n",
        "- In Apache Spark, transformations are operations that create a new RDD (Resilient Distributed Dataset) from an existing one. There are two types of transformations in Spark: Narrow and Wide.\n",
        "\n",
        "In general, it is recommended to use narrow transformations whenever possible to optimize performance and minimize data movement across the cluster. However, there are cases where wide transformations are necessary to achieve the desired computation, such as aggregations or joins that require combining data across multiple partitions.\n",
        "\n",
        "In summary, narrow transformations are operations where each input partition is used to compute one output partition, while wide transformations are operations where each input partition is used to compute multiple output partitions. Narrow transformations are preferred because they are more efficient and require less data movement across the cluster, but there are cases where wide transformations are necessary for certain types of computations."
      ],
      "metadata": {
        "id": "w2z3lmvV1zyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#17. mapPartitionsWithIndex"
      ],
      "metadata": {
        "id": "JKyUWyZ8B9eR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "syntax: RDD.mapPartitionsWithIndex(f: Callable[[int, Iterable[T]], Iterable[U]], preservesPartitioning: bool = False)"
      ],
      "metadata": {
        "id": "k10BdvgC_8Be"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition."
      ],
      "metadata": {
        "id": "J3f-9gtj_xy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#18.groupby"
      ],
      "metadata": {
        "id": "yZltdG6rCMYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to SQL GROUP BY clause, PySpark groupBy() function is used to collect the identical data into groups on DataFrame and perform count, sum, avg, min, max functions on the grouped data. In this article, I will explain several groupBy() examples using PySpark\n"
      ],
      "metadata": {
        "id": "Fj-zZ4q2D6kK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19. keyBy\n",
        "keyBy(f: Callable[[T], K])\n",
        "f: function\n",
        "a RDD with the elements from this that are not in other"
      ],
      "metadata": {
        "id": "HMyZTlWmZEZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax\n",
        "DataFrame.groupBy(*cols)\n",
        "or\n",
        "DataFrame.groupby(*cols)\n"
      ],
      "metadata": {
        "id": "m1baoT01EaYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "keyBy: generally to generate key for each row"
      ],
      "metadata": {
        "id": "b-Z-j-6OZ3fz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#20. zip"
      ],
      "metadata": {
        "id": "bXWiQ3TIcoK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntax: RDD.zip(other: pyspark.rdd.RDD[U])\n",
        "other:RDD"
      ],
      "metadata": {
        "id": "ZwgAUBFOdTmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "merge two RDDs and key-value pair."
      ],
      "metadata": {
        "id": "DyCfiyr6cqxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pandas----->single cluster node.\n",
        "Pyspark---->Multiple cluster node.\n"
      ],
      "metadata": {
        "id": "fHEQpO85dBiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#21. Zip-WithIndex"
      ],
      "metadata": {
        "id": "ZBSKpDqKdbHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD.zipWithIndex() → pyspark.rdd.RDD[Tuple[T, int]]\n",
        "Zips this RDD with its element indices.\n",
        "\n",
        "The ordering is first based on the partition index and then the ordering of items within each partition. So the first item in the first partition gets index 0, and the last item in the last partition receives the largest index.\n",
        "\n",
        "This method needs to trigger a spark job when this RDD contains more than one partitions."
      ],
      "metadata": {
        "id": "vtCnsb3Xd9le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#22. repartition\n",
        "pyspark.sql.DataFrame.repartition() method is used to increase or decrease the RDD/DataFrame partitions by number of partitions or by single column name or multiple column names. This function takes 2 parameters; numPartitions and *cols, when one is specified the other is optional. repartition() is a wider transformation that involves shuffling of the data hence, it is considered an expensive operation."
      ],
      "metadata": {
        "id": "m8dfvl47f686"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Points –\n",
        "\n",
        "repartition() is used to increase or decrease the number of partitions.\n",
        "repartition() creates even partitions when compared with coalesce().\n",
        "It is a wider transformation.\n",
        "It is an expensive operation as it involves data shuffle and consumes more resources.\n",
        "repartition() can take int or column names as param to define how to perform the partitions.\n",
        "If parameters are not specified, it uses the default number of partitions.\n",
        "As part of performance optimization, recommends avoiding using this function."
      ],
      "metadata": {
        "id": "lHvhQsBzgG2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "repartition() versus coalesce()\n",
        "The repartition() can be used to increase or decrease the number of partitions, but it involves heavy data shuffling across the cluster. On the other hand, coalesce() can be used only to decrease the number of partitions. In most of the cases, coalesce() does not trigger a shuffle."
      ],
      "metadata": {
        "id": "wRR286E3kr3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "coalesc still faster"
      ],
      "metadata": {
        "id": "y5CtPk0ikwYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD is not expanded\n",
        "partition is expanded"
      ],
      "metadata": {
        "id": "nm8YQtYwk5bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#23. withColumn"
      ],
      "metadata": {
        "id": "HfGZiLm7uRIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PySpark withColumn() is a transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more. In this post, I will walk you through commonly used PySpark DataFrame column operations using withColumn() examples."
      ],
      "metadata": {
        "id": "Fjgm-x-ZuUEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Action operations\n",
        "    It doent create any RDD\n",
        "        collect\n",
        "        count\n",
        "        first\n",
        "        take\n",
        "        top\n",
        "        countByValue\n",
        "        reduce\n",
        "        fold\n",
        "        aggregate\n",
        "        foreach\n",
        "        saveAsTextFile"
      ],
      "metadata": {
        "id": "uN8zM7lp390k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#24. Reduce\n",
        "it traverse each row and aggregate"
      ],
      "metadata": {
        "id": "VNNjV6tyuWxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#25. First\n",
        "only first element"
      ],
      "metadata": {
        "id": "NRUXgyHZ4euk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#26.TakeOrdered\n",
        "take order it and send it"
      ],
      "metadata": {
        "id": "qe2sjoqs4zhT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#27. take\n",
        "it is original order"
      ],
      "metadata": {
        "id": "sgfCLtlK5AXk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#28. count"
      ],
      "metadata": {
        "id": "DeOtYvKi5FIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#29. collectAsMap\n",
        "it returns dictionary key-values pair"
      ],
      "metadata": {
        "id": "nmOX7a3a5MWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#30. saveAsTextFile\n"
      ],
      "metadata": {
        "id": "WDf0Z0Si6QeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#31. saveAsPickleFile"
      ],
      "metadata": {
        "id": "rXsL7ePy6dMU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "picklefile is create binary"
      ],
      "metadata": {
        "id": "z73tM4IL6b15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#32. Foreach\n"
      ],
      "metadata": {
        "id": "v_ffIbLd6rnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "final result oriented operation"
      ],
      "metadata": {
        "id": "drvTiuto8hC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#33. Foreach-Partition\n",
        "it is similar to map but Foreach is an final action operation.\n",
        "In Spark foreachPartition() is used when you have a heavy initialization (like database connection) and wanted to initialize once per partition where as foreach() is used to apply a function on every element of a RDD/DataFrame/Dataset partition\n"
      ],
      "metadata": {
        "id": "6Oi27DYB9-A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#35.countByValue\n",
        "Return the count of each unique value in this RDD as a dictionary of (value, count) pairs."
      ],
      "metadata": {
        "id": "6FolCaGBEMGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#36. PySpark – Math Functions\n",
        "these are RDD operations not python operation"
      ],
      "metadata": {
        "id": "3fZdl-OIDNAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.sum\n",
        "2.min\n",
        "3.variance()\n",
        "4.max()\n",
        "5.mean()\n",
        "6.stdev()"
      ],
      "metadata": {
        "id": "cukA3j0aDo5I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#37. toDebugString\n",
        "A description of this RDD and its recursive dependencies for debugging."
      ],
      "metadata": {
        "id": "Ei1JaIQgEyA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#38. printSchema"
      ],
      "metadata": {
        "id": "fwb30z3VFIEY"
      }
    }
  ]
}